Title: CLAR: Contrastive Learning of Auditory Representations

Representation learning is a crucial component in the wide success of deep learning algorithms by disentangling compact independent high-level factors from low-level sensory data. Recently, contrastive self-supervised learning has been successful in learning-rich visual representations by leveraging the inherent structure of unlabeled images. However, it is still unclear whether we could use a similar self-supervised approach to learn superior auditory representations. In this work, we expand on prior studies on contrastive self-supervised learning (SimCLR) to learn rich auditory representations. We evaluated the performance of our proposed framework on three different audio datasets from diverse domains (speech, music, and environmental sounds). Furthermore, we trained two families of deep residual neural networks, one type of network were trained on raw audio signals while the other type utilized time-frequency audio features. In this work, we (1) introduced and evaluated the impact of various auditory data augmentations on predictive performance, (2) showed that training with time-frequency audio features substantially improves the quality of the learned representations compared to raw signals, and (3) demonstrated that simultaneous training with both supervised and contrastive losses improves the learned representations compared to self-supervised and supervised training. We illustrated that by combining all these methods and with substantially less labeled data, our framework (CLAR) achieves significant improvement on prediction performance compared to self-supervised (11.3% improvement) and supervised (1% improvement) approaches. Moreover, while the self-supervised approach converges to > 80% predictive performance within approximately 500 epochs, our framework converges to > 90% within approximately 30 epochs.

Publication Status of the Work: Published

Link to Full Paper (if available): https://arxiv.org/abs/2010.09542

