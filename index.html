<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0">
        <title>OWCV 2021</title>
        <link rel="stylesheet" href="style.css">
        <link rel="stylesheet" href="schedule.css">
        <link href="https://fonts.googleapis.com/css2?family=Commissioner&family=Lato&display=swap" rel="stylesheet">
    </head>
    <body>
        <div id='nav-container'>
            <nav>
                <a href='#about'>About</a>
                <a href='#dates'>Dates</a>
                <a href='#cfp'>CfP</a>
                <a href='#keynote'>Speakers</a>
                <a href='#schedule'>Schedule</a>
                <a href='#papers'>Papers</a>
                <a href='#org'>Organizers</a>
                <a href='#contact'>Contact</a>
            </nav>
        </div>
        <div id='canvas-container' class='full-bleed'>
            <!-- The following h1 and h2 elements are read from the javascript code to generate the 3D scene -->
            <h1>OWCV2021</h1>
            <h2>(Virtual)</h2>
            <h2>April 21-22, 2021</h2>
            <div class='btn-container'>
                <div class='button register' onclick="window.open('https://docs.google.com/forms/d/10MTPsV8BzL_doKnbnSuuN8fM3puAV15SJoobSp2EXXo/viewform')">Register Now</div>
                <div onclick='window.location="#about"' class='button more-info'>More info â†“</div>
            </div>
        </div>
        <div id='about'>
            <h2>About the event</h2>
            <p>Ontario Workshop on Computer Vision (OWCV) is a computer vision workshop, organized by and for the major computer vision labs in Ontario. It is an opportunity for students to present their prospective submissions to a friendly, constructively critical audience, interact with fellow researchers, and to get valuable feedback.</p>
            <div class='btn-container full-bleed'>
                <div onclick='window.location="#schedule"' class='button schedule'>Schedule</div>
                <!-- <div class='button register' onclick="window.open('https://docs.google.com/forms/d/10MTPsV8BzL_doKnbnSuuN8fM3puAV15SJoobSp2EXXo/viewform')">Register</div> -->
            </div>
            <br><br><br>
            <div>
                <p>Follow us on <a href="https://twitter.com/_OWCV" target="_blank">Twitter</a>!</p>
                <a class="twitter-timeline" data-height="420" data-size="12px" data-theme="dark" href="https://twitter.com/_OWCV?ref_src=twsrc%5Etfw">Tweets by _OWCV</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
            </div>
        </div>
        <div id='dates'>
            <h2>Important Dates</h2>
            <p><strike>Submission deadline : <b>April 5, 2021 11:59pm</b> Toronto time</strike></p>
            <p><strike>Registration deadline : <b>April 10, 2021 11:59pm</b> Toronto time</strike></p>
            <p>Formal registration is closed. But if you're still interested in attending, please email owcv2021@gmail.com!</p>
        </div>
        <div id='cfp'>
            <h2>Call for Papers</h2>
            <ul class="cfp">
                <li><b>Abstract only</b>: 2500 characters maximum, no published proceedings.</li>
                <li><b>Share your work</b>: We accept already published material which you would like to highlight for the Ontario computer vision community.</li>
                <li><b>Solicit feedback</b>: Unpublished work and works in progress are also accepted. Note that while sufficient material should be completed for an engaging poster or talk, it does not need to be ready for full publication.</li>
                <li><b>Topics of Interest</b>: We are interested in all areas pertaining to computer vision, including but not limited to:
                <table style="width: 100%;">
                <tbody>
                    <tr>
                    <td style="width: 45.0000%;">
                        <ul>
                            <li>3D Vision</li>
                            <li>Biomedical Imaging</li>
                            <li>Computational Photography</li>
                            <li>Ethics in Computer Vision</li>
                            <li>Datasets and Evaluation</li>
                            <li>Deep Learning</li>
                            <li>Generative Models</li>
                            <li>Geometry in Vision</li>
                            <li>Image Synthesis</li>
                        </ul>
                    </td>
                    <td style="width: 55.0000%;">
                        <ul>
                            <li>Image Processing
                            <li>Image Understanding</li>
                            <li>Object Tracking</li>
                            <li>Pose Estimation</li>
                            <li>Scene Analysis</li>
                            <li>Spatiotemporal Vision</li>
                            <li>Vision + Other Modalities</li>
                            <li>Visual Attention</li>
                            <li>Vision for Robotics</li>
                        </ul>
                    </td>
                    </tr>
                </tbody>
                </table>
                </li>
            </ul>
        </div>
        <div id='keynote'>
            <h2>Keynote Speakers</h2>
            <div class='speaker-container full-bleed'>
                <div>
                    <a href="http://www.cse.yorku.ca/~mbrown/", target="_blank">
                        <h3>Michael Brown</h3>
                        <img src='res/michael.jpg' />
                    </a>
                    <p>Canada Research Chair in Computer Vision<br>Dept. of Electrical Engineering and Computer Science, York University</p>
                </div>
                <div class="speaker-content">
                    <br><br>
                    <p><em>Title</em>: <b>In-Camera Color Processing</b></p>
                    <p><em>Abstract</em>: This is a two-part presentation. The first part will provide an overview of how your digital camera processes the sensor image (RAW) to the final output image (sRGB-JPEG). The second part of the presentation will discuss research targeting various aspects of the in-camera processing pipeline, including demosaicing, denoising, white-balance, and general color processing.</p>
                </div>
            </div>
            <div class='speaker-container full-bleed'>
                <div>
                    <a href="https://www.cs.utoronto.ca/~fidler/", target="_blank">
                        <h3>Sanja Fidler</h3>
                        <img src='res/sanja.jpg' />
                    </a>
                    <p>Associate Professor, University of Toronto<br>Director of AI, NVIDIA<br>Vector Institute (co-founder)</p>
                </div>
            </div>
            <div class='speaker-container full-bleed'>
                <div>
                    <a href="https://www.gwtaylor.ca/", target="_blank">
                        <h3>Graham Taylor</h3>
                        <img src='res/graham.jpg' />
                    </a>
                    <p>Associate Professor and Canada Research Chair in Machine Learning<br>Canada CIFAR AI Chair<br>School of Engineering, University of Guelph and Vector Institute<br>Academic Director, NextAI</p>
                </div>
                <div class="speaker-content">
                    <br>
                    <p><em>Title</em>: <b>Advances in Conditional Generative Models</b></p>
                    <p><em>Abstract</em>: In this talk, I will provide an overview of my group's recent work in the domain of conditional generative models. Conditional generative models take some context (the condition) and perform controlled synthesis: text, images, or some other kind of structured output. They are computationally intensive, unwieldy to train and we struggle to evaluate them, given the subjective nature of their creations.</p>
                    <p>With Facebook AI Research (FAIR) we investigated ways to quantitatively evaluate generative models' outputs in a way that captures quality, diversity and consistency with the instructions we provide them. With FAIR we also showed that more data is not always better when it comes to generative models: a novel automatic data selection process can make training easier and resulting models more robust, with little reduction of diversity. Moreover, this technique massively reduces the computational requirements, making large-scale models more accessible to a wider range of users. In a recent collaboration with Microsoft Research and Mila we advocated for a departure from characteristic "single-shot" type generation. We proposed a method for iterative image generation, inspired by the way a sketch artist composes a scene. Departing from pixel-based outputs, I will also discuss graph-based generative models for constructing physical assemblies, starting with the children's toy LEGO.</p>
                </div>
            </div>
            <div class='speaker-container full-bleed'>
                <div>
                    <a href="http://www.eng.uwaterloo.ca/~a28wong/", target="_blank">
                        <h3>Alexander Wong</h3>
                        <img src='res/alex.jpg' />
                    </a>
                    <p>Canada Research Chair in Artificial Intelligence and Medical Imaging<br>Member, College of the Royal Society of Canada<br>Associate Professor, P.Eng.<br>Co-Director, <a href="https://uwaterloo.ca/vision-image-processing-lab/", target="_blank">Vision and Image Processing (VIP) Research Group</a><br>Department of Systems Design Engineering, University of Waterloo</p>
                </div>
                <div class="speaker-content">
                    <br>
                    <p><em>Title</em>: <b>Road to Operational AI: Challenges and Opportunities</b></p>
                    <p><em>Abstract</em>: Tremendous advances in artificial intelligence over the past two decades have led to a significant interest in leveraging artificial intelligence across enterprises and industries.  However, despite these tremendous opportunities and the significant successes experienced by large, technology enterprises in reaping the benefits of artificial intelligence in delivering better solutions with great value, the widespread adoption of operational artificial intelligence has seen very limited success in many industries and scenarios.  In fact, even early successes in adoption have started to surface additional issues that hinder further deployment and adoption. In this talk, I will discuss some of the key challenges in the operationalization of artificial intelligence, ranging from scalability to trust and dependability, and some potential solutions for addressing these challenges.</p>
                </div>
            </div>
        </div>
        <div id='schedule'>
            <h2>Schedule</h2>
            <div id='schedule-grid-container'>
                <div id='schedule-grid'>
                    <!-- See schedule.js for how this section is automatically filled using schedule.json -->
                </div>
            </div>
            <!-- <p> Coming soon! </p> -->
        </div>
        <div id='papers'>
            <h2>Accepted Papers</h2>
            <table style="width: 100%;">
            <tbody>
                <tr>
                    <th>Day 1</th>
                    <th>Day 2</th>
                </tr>
                <tr style="vertical-align:top">
                <td style="width: 50.0000%;">
                    <ul style="list-style-type:none; text-align:left">
                        <li>1. <b>CLAR: Contrastive Learning of Auditory Representations</b> - Haider Al-Tahan, Yalda Mohsenzadeh</li>
                        <li>3. <b>Automatic Abbreviation of Hockey Videos</b> - Hemanth Pidaparthy, Michael Dowling, James Elder</li>
                        <li>5. <b>Understanding Gene Model Maps</b> - Michael Lombardo, Faisal Qureshi</li>
                        <li>7. <b>Identifying and interpreting tuning dimensions in deep networks</b> - Nolan S. Dey, J. Eric Taylor, Bryan P. Tripp, Alexander Wong, Graham W. Taylor</li>
                        <li>9. <b>Zero-shot Learning with Class Description Regularization</b> - Shayan Kousha, Marcus A. Brubaker</li>
                        <li>11. <b>Active Correspondences Projector-Camera (Procam) Sensors</b> - Jonathon Malcolm, Ian J. Maquignaz</li>
                        <li>13. <b>Normalizing Flow for semi-supervised learning</b> - Vincent Sham</li>
                        <li>15. <b>Wavelet Flow: Fast Training of High Resolution Normalizing Flows</b> - Jason J. Yu, Konstantinos G. Derpanis, Marcus A. Brubaker</li>
                        <li>17. <b>Hyperspectral pixel unmixing with Deep Variational Inference</b> - Kiran Mantripragada, Faisal Z. Qureshi</li>
                        <li>19. <b>Training neural networks for feature alignment</b> - Siavash Rezaei</li>
                        <li>21. <b>SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation</b> - Brendan Duke, Abdalla Ahmed, Christian Wolf, Parham Aarabi, Graham W. Taylor</li>
                        <li>23. <b>Unsupervised Image Demoireing using Implicit Neural Representation</b> - Seonghyeon Nam, Marcus A. Brubaker, Michael S. Brown</li>
                        <li>25. <b>Precisely calibrated and spatially informed illumination for conventional fluorescence and improved PALM imaging applications</b> - Angel Mancebo, Luke DeMars, Christopher T Ertsgaard, Elias M Puchner</li>
                        <li>27. <b>Topo Sampler: A Topology Constrained Noise Sampling for GANs</b> - Adrish Dey, Sayantan Das</li>
                    </ul>
                </td>
                <td style="width: 50.0000%;">
                    <ul style="list-style-type:none; text-align:left">
                        <li>2. <b>Transformer-Based Network For Image Operator Approximation</b> - Ian MacPherson
                        <li>4. <b>Contrastive Learning for Sports Video: Unsupervised Player Classification</b> - Maria Koshkina, Hemanth Pidaparthy, James Elder</li>
                        <li>6. <b>Renal Boundary and Tumour Detection in MRI using U-Net with Transfer Learning</b> - Anush Agarwal, Nicola Schieda, Mohamed Elfaal, Eranga Ukwatta</li>
                        <li>8. <b>OLED: One-Class Learned Encoder-Decoder Network with Adversarial Context Masking for Novelty Detection</b> - John Taylor Jewell, Vahid Reza Khazaie, Yalda Mohsenzadeh</li>
                        <li>10. <b>Object completion with stochastic completion fields</b> - Morteza Rezanejad, Sidharth Gupta, Chandra Gummaluru, Ryan Marten, John Wilder, Michael Gruninger, Dirk Walther</li>
                        <li>12. <b>Deep Learning-Based Segmentation of Neonatal Cerebral Lateral Ventricles from 3D Ultrasound Images</b> - Zachary Szentimrey, Sandrine de Ribaupierre, Aaron Fenster, Eranga Ukwatta</li>
                        <li>14. <b>Scale and translation  invariance preserving operators on distributions of images</b> - Xavier Snelgrove, Sven Dickinson, Marcus Brubaker</li>
                        <li>16. <b>Noise2NoiseFlow: Learning Camera Noise Models without Corresponding Clean Images</b> - Ali Maleky, Michael S. Brown, Marcus A. Brubaker</li>
                        <li>18. <b>Procam Calibration from a Single Pose of a Planar Target</b> - Ghani O. Lawal, Michael Greenspan</li>
                        <li>20. <b>Structured Visual Search via Composition-aware Learning</b> - Mert Kilickaya, Arnold Smeulders</li>
                        <li>22. <b>Learning by Aligning Videos in Time</b> - Sanjay Haresh, Sateesh Kumar, Huseyin Coskun, Shahram N. Syed, Andrey Konin, M. Zeeshan Zia, Quoc-Huy Tran</li>
                        <li>24. <b>A study on the effects of compression on hyperspectral image classification</b> - Kiran Mantripragada, Faisal Z. Qureshi, Phuong D. Dao, Yuhong He</li>
                        <li>26. <b>Improved 6 DOF Pose Estimation Using Post-Processing Validation</b> - Joy Mazumder, Mohsen Zand, Michael Greenspan</li>
                    </ul>
                </td>
                </tr>
            </tbody>
            </table>
        </div>
        <div id='org'>
            <h2>Organizers</h2>
            <div class='speaker-container full-bleed'>
                <div>
                    <h3><u>Organizing Co-Chair</u></h3>
                    <a href='https://mkowal2.github.io/', target="_blank", style="color:#FFFFFF;">
                    <img src='res/matt.png' />
                    <h3><b>Matt Kowal</b></h3>
                    </a>
                    <p>PhD student, Ryerson University<br>Postgraduate Affiliate, Vector Institute</p>
                </div>
                <div>
                    <h3><u>Organizing Co-Chair</u></h3>
                    <a href='https://voletiv.github.io/', target="_blank", style="color:#FFFFFF;">
                    <img src='res/vikram.jpg' />
                    <h3><b>Vikram Voleti</b></h3>
                    </a>
                    <p>PhD candidate, Mila<br>Visiting Researcher, University of Guelph</p>
                </div>
            </div>
            <div class='speaker-container full-bleed'>
                <div>
                    <h3><u>Program Chair</u></h3>
                    <a href='http://www.cse.yorku.ca/~calden/', target="_blank", style="color:#FFFFFF;">
                    <img src='res/calden.jpg' />
                    <h3><b>Calden Wloka</b></h3>
                    </a>
                    <p>Post-doctoral Visitor,<br>York University</p>
                </div>
                <div>
                    <h3><u>General Chair</u></h3>
                    <a href='https://mbrubake.github.io/', target="_blank", style="color:#FFFFFF;">
                    <img src='res/marcus.png' />
                    <h3><b>Marcus Brubaker</b></h3>
                    </a>
                    <p>Assistant Professor, York University<br>Faculty Affiliate, Vector Institute<br>Adjunct Professor, University of Toronto</p>
                </div>
            </div>
            <h3>Program Committee</h3>
            <div class='speaker-container full-bleed'>
                <div>
                    <a href='https://sites.google.com/view/abdullah-abuolaim/', target="_blank", style="color:#FFFFFF;">
                    <img src='res/abdullah.png' />
                    <h3><b>Abdullah Abuolaim</b></h3>
                    </a>
                    <p>PhD candidate,<br>York University</p>
                </div>
                <div>
                    <a href='http://www.cse.yorku.ca/~yulia_k/', target="_blank", style="color:#FFFFFF;">
                    <img src='res/iuliia.jpg' />
                    <h3><b>Iuliia Kotseruba</b></h3>
                    </a>
                    <p>PhD student,<br>York University</p>
                </div>
            </div>
        </div>
        <div id='contact'>
            <h2>Contact</h2>
            <p>Any questions at all? Send us an email!</p>
            <div class='btn-container full-bleed'>
                <div class='button contact' onclick="window.open('mailto:owcv2021@gmail.com')" title='owcv2021@gmail.com'>Contact</div>
            </div>
        </div>
        <div id='Thanks'>
            <p>Thanks to <a href="https://graphquon.github.io/", target="_blank">graphquon.github.io</a> for the website theme!</p>
        </div>

        <script src="lib/three.min.js"></script>
        <script src="lib/OrbitControl.js"></script>
        <script src="https://unpkg.com/ionicons@5.2.3/dist/ionicons.js"></script>

        <script src='index.js'></script>
        <script src="schedule.js"></script>
    </body>
</html>
